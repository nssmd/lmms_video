#!/usr/bin/env python3
"""
ç›´æ¥ç»Ÿè®¡è‡ªå®šä¹‰ Qwen2.5-VL æ¨¡å‹çš„å®é™…å‚æ•°æ•°é‡
"""

import torch
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹çš„å‚æ•°æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    
    return total_params / 1e6

def create_qwen2_model(config):
    """
    æ ¹æ®é…ç½®åˆ›å»º Qwen2 æ¨¡å‹å¹¶ç»Ÿè®¡å‚æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
    
    Returns:
        æ€»å‚æ•°æ•°é‡ (å•ä½: M)
    """
    try:
        from transformers import Qwen2Config, Qwen2ForCausalLM
        
        print(f"=== æ¨¡å‹é…ç½® ===")
        print(f"æ¨¡å‹ç±»å‹: Qwen2 (æ–‡æœ¬æ¨¡å‹)")
        print(f"vocab_size: {config['vocab_size']}")
        print(f"hidden_size: {config['hidden_size']}, layers: {config['num_hidden_layers']}")
        print()
        
        # åˆ›å»ºé…ç½®
        model_config = Qwen2Config(**config)
        
        # åˆ›å»ºæ¨¡å‹ (ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡)
        print("æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
        model = Qwen2ForCausalLM(model_config)
        
        print("æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹ç»Ÿè®¡å‚æ•°...")
        
        # åˆ†æ¨¡å—ç»Ÿè®¡å‚æ•°
        print(f"\n=== åˆ†æ¨¡å—å‚æ•°ç»Ÿè®¡ ===")
        
        # 1. è¯åµŒå…¥å±‚
        if hasattr(model.model, 'embed_tokens'):
            embed_params = sum(p.numel() for p in model.model.embed_tokens.parameters())
            print(f"è¯åµŒå…¥å±‚: {embed_params:,} ({embed_params/1e6:.2f}M)")
        
        # 2. Transformerå±‚
        if hasattr(model.model, 'layers'):
            layer_params = sum(p.numel() for p in model.model.layers.parameters())
            print(f"Transformerå±‚: {layer_params:,} ({layer_params/1e6:.2f}M)")
        
        # 3. è¾“å‡ºå±‚
        if hasattr(model, 'lm_head'):
            lm_head_params = sum(p.numel() for p in model.lm_head.parameters())
            print(f"è¾“å‡ºå±‚(lm_head): {lm_head_params:,} ({lm_head_params/1e6:.2f}M)")
        
        # 4. å…¶ä»–æ¨¡å—
        other_params = 0
        for name, module in model.named_children():
            if name not in ['model', 'lm_head']:
                module_params = sum(p.numel() for p in module.parameters())
                if module_params > 0:
                    print(f"{name}: {module_params:,} ({module_params/1e6:.2f}M)")
                    other_params += module_params
        
        print(f"\n=== æ€»å‚æ•°ç»Ÿè®¡ ===")
        total_params = count_parameters(model)
        
        return total_params
        


# æ›´æ–°åçš„ä¸‰ç§é…ç½® (ä½¿ç”¨LLaMA tokenizerçš„Qwen2æ¨¡å‹)
configs = {
    "60M": {
        'vocab_size': 32000,  # LLaMA tokenizer
        'max_position_embeddings': 4096,
        'rope_theta': 10000.0,
        'hidden_size': 512,
        'intermediate_size': 1376,
        'num_hidden_layers': 8,
        'num_attention_heads': 8,
        'num_key_value_heads': 8,
        'tie_word_embeddings': False,
        'use_cache': True,
        'attention_dropout': 0.0,
        'hidden_dropout': 0.0,
        'initializer_range': 0.02
    },
    "130M": {
        'vocab_size': 32000,  # LLaMA tokenizer
        'max_position_embeddings': 4096,
        'rope_theta': 10000.0,
        'hidden_size': 768,
        'intermediate_size': 2048,
        'num_hidden_layers': 12,
        'num_attention_heads': 12,
        'num_key_value_heads': 2,  # GQA
        'tie_word_embeddings': False,
        'use_cache': True,
        'attention_dropout': 0.0,
        'hidden_dropout': 0.0,
        'initializer_range': 0.02
    },
    "350M": {
        'vocab_size': 32000,  # LLaMA tokenizer
        'max_position_embeddings': 4096,
        'rope_theta': 10000.0,
        'hidden_size': 1024,
        'intermediate_size': 2736,
        'num_hidden_layers': 24,
        'num_attention_heads': 16,
        'num_key_value_heads': 2,  # GQA
        'tie_word_embeddings': False,
        'use_cache': True,
        'attention_dropout': 0.0,
        'hidden_dropout': 0.0,
        'initializer_range': 0.02
    }
}

if __name__ == "__main__":
    print("ç»Ÿè®¡ Qwen2 æ¨¡å‹å‚æ•° (ä½¿ç”¨LLaMA tokenizer)")
    print("é¦–å…ˆå°è¯•åˆ›å»ºçœŸå®æ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–æ¨¡å‹ä¼°ç®—")
    print()
    
    for model_name, config in configs.items():
        print(f"\n{'='*60}")
        print(f"ç»Ÿè®¡ {model_name} æ¨¡å‹å‚æ•°")
        print(f"{'='*60}")
        
        # é¦–å…ˆå°è¯•åˆ›å»ºçœŸå®æ¨¡å‹
        total_params = create_qwen2_model(config)
        
        if total_params is not None:
            print(f"\nğŸ¯ {model_name} æ¨¡å‹æ€»å‚æ•°: {total_params:.2f}M")
        else:
            print(f"\nâŒ {model_name} æ¨¡å‹å‚æ•°ç»Ÿè®¡å¤±è´¥")
        print()
