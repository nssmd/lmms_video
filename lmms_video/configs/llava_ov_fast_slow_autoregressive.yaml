# LLaVA-OneVision + å¿«æ…¢å¸§è‡ªå›å½’è®­ç»ƒé…ç½®
# ä½¿ç”¨æœ¬åœ° Parquet æ ¼å¼çš„ LLaVA-Video-178K æ•°æ®é›†
# é›†æˆå¿«æ…¢å¸§æœºåˆ¶å’Œè‡ªå›å½’è§†é¢‘é‡å»º

- type: trainer
  config:
    trainer_type: autoregressive_trainer

    # ==================== æ•°æ®é›†é…ç½® ====================
    dataset_config:
      dataset_type: vision
      dataset_format: parquet  # ä½¿ç”¨ parquet æ ¼å¼

      # Parquet æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
      dataset_path: "/blob/LLaVA-Video-178K-parquet/*.parquet"
      data_folder: /blob/LLaVA-Video-178K-parquet

      # è§†é¢‘å¤„ç†å‚æ•°
      max_frames_num: 16
      fps: 1.0
      force_sample: true
      video_decode_backend: "decord"

      # Processor é…ç½®
      processor_config:
        processor_name: "lmms-lab/llava-onevision-qwen2-7b-ov"
        processor_type: "llava_onevision"
        max_pixels: 1003520
        min_pixels: 4096

      # Packing é…ç½®
      packing: false

    # ==================== æ¨¡å‹é…ç½® ====================
    model_config:
      # LLaVA-OneVision åŸºç¡€æ¨¡å‹
      load_from_pretrained_path: "lmms-lab/llava-onevision-qwen2-7b-ov"
      attn_implementation: "flash_attention_2"
      vision_tower: "google/siglip-so400m-patch14-384"

      # ğŸ”¥ å¯ç”¨è‡ªå›å½’é‡å»º
      enable_autoregressive: true

      # ğŸ”¥ğŸ”¥ å¿«æ…¢å¸§ + è‡ªå›å½’é…ç½®
      autoregressive_config:
        # åŸºç¡€å‚æ•°
        embedding_dim: 1152
        hidden_size: 3584
        loss_weight: 0.15
        num_heads: 8
        num_layers: 3

        # å¿«æ…¢å¸§æ ¸å¿ƒå‚æ•°
        mm_spatial_pool_stride: 4
        mm_spatial_pool_mode: "average"
        frame_sampling_strategy: "interleave"
        slow_frame_ratio: 0.5

        # å¸§çº§åˆ«å› æœmask
        use_frame_causal_mask: true

        # å¿«æ…¢å¸§æ—¶åºé‡‡æ ·
        use_fast_slow_frames: true
        fast_stride: 1
        slow_stride: 4
        num_fast: 8
        num_slow: 8

        # LLaVA-NeXT å¤šå°ºåº¦ç‰¹æ€§
        enable_multiscale_pooling: true
        add_faster_video: true

    # ==================== è®­ç»ƒå‚æ•° ====================
    output_dir: "./output/llava_ov_fast_slow_autoregressive"
    run_name: "llava_ov_fast_slow_ar"

    num_train_epochs: 3
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 8

    learning_rate: 2.0e-5
    weight_decay: 0.01
    warmup_ratio: 0.03
    lr_scheduler_type: "cosine"
    optim: "adamw_torch"

    bf16: true
    tf32: true
    fp16: false

    max_grad_norm: 1.0
    gradient_checkpointing: true

    # ==================== æ—¥å¿—å’Œä¿å­˜ ====================
    logging_steps: 10
    save_strategy: "steps"
    save_steps: 500
    save_total_limit: 3
    evaluation_strategy: "no"

    logging_dir: "./logs"
    report_to: ["tensorboard"]
    logging_first_step: true

    # ==================== æ•°æ®åŠ è½½ ====================
    dataloader_num_workers: 4
    dataloader_pin_memory: true
    remove_unused_columns: false
    dataloader_persistent_workers: true

    # ==================== å…¶ä»– ====================
    seed: 42
    ddp_find_unused_parameters: false
    output_hidden_states: true
    model_max_length: 2048
    padding_side: "right"

    # Vision é…ç½®
    image_aspect_ratio: "anyres_max_9"
    mm_patch_merge_type: "spatial_unpad"
    mm_newline_position: "grid"

    modality: "video"
    use_video: true